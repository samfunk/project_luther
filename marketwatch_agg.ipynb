{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape every MarketWatch article from June - September"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses selenium and BeautifulSoup to scrape every marketwatch.com news article from June through September 2017. It first scrapes all the links and titles and then segments the articles based on the section they appeared in, specifically looking for articles related to economic and Federal Reserve news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "chromedriver = '/Applications/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use selenium to remotely navigate marketwatch.com and set search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize marketwatch.com\n",
    "url = 'http://www.marketwatch.com'\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(url)\n",
    "\n",
    "#Advanced search\n",
    "path = '//a[@class=\"btn btn--outline btn--search\"]'\n",
    "driver.find_element_by_xpath(path).click()\n",
    "driver.find_element_by_xpath('//a[text()=\"Advanced Search\"]').send_keys(Keys.RETURN)\n",
    "\n",
    "#Advanced Search\n",
    "driver.find_element_by_id('refinesearchtoggle').click()\n",
    "driver.find_element_by_xpath('//input[@type=\"checkbox\"]').click()\n",
    "\n",
    "#Set Subject\n",
    "search_mode = '//select[@id=\"mp\"]/option[@value=\"806\"]'\n",
    "driver.find_element_by_xpath(search_mode).click()\n",
    "\n",
    "#Results Per Page\n",
    "results = '//select[@id=\"rpp\"]/option[@value=\"100\"]'\n",
    "driver.find_element_by_xpath(results).click()\n",
    "\n",
    "#Set date\n",
    "date = driver.find_element_by_id('bdv')\n",
    "date.click()\n",
    "date.send_keys('09/30/2017')\n",
    "\n",
    "#Search\n",
    "driver.find_element_by_xpath('//input[@value=\"Search\"]').click()\n",
    "current_url = driver.current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Should be for {Keyword: All MarketWatch, Results Per Page: 100, News On Or Before: 9/30/2017}\n",
    "#Only run if you have run the above code to get the 'current_url'\n",
    "current_url = 'http://www.marketwatch.com/search?q=&m=Keyword&rpp=100&mp=806&bd=true&bd=false&bdv=09%2F30%2F2017&rs=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that scrapes each article's link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    '''\n",
    "    Get the title, url, and date of each article\n",
    "    MUST CHECK THE DICTIONARY AT THE END OF THE LOOP\n",
    "    ---\n",
    "    IN: specific url\n",
    "    OUT: date\n",
    "    '''\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    pages = soup.find_all(class_='searchresult')\n",
    "    for div in pages:\n",
    "        attrs = dict()\n",
    "        if div.find('a', href=True):\n",
    "            if div.a.parent.name == 'div':\n",
    "                attrs['url'] = div.a['href']\n",
    "                attrs['date'] = div.next_sibling.span.text\n",
    "                econ[div.a.text] = attrs  #MUST CHECK THIS DICTIONARY\n",
    "    return pages[0].next_sibling.span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(date):\n",
    "    '''\n",
    "    Properly format date from scraped date\n",
    "    '''\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation.replace(':', '')))\n",
    "    date = regex.sub('', date)\n",
    "    date = re.search(r'([A-Z].*)', date)[1]\n",
    "    date = re.sub(r'([A-Za-z]{3})[a-z]*', r'\\1', date)\n",
    "    date = datetime.strptime(date, '%b %d %Y')\n",
    "    #date = date.strftime('%m/%d/%Y')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuously scrape search result pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def continuous_search(month, current_url, stop_month):\n",
    "    '''\n",
    "    This function uses selenium and the above functions to scrape each article link from each search page,\n",
    "    when the search results run out or if there are not links present, then the function will take the most recent date,\n",
    "    and input it into the date field and start a new search and continue to loop through each page until it reaches May\n",
    "    ---\n",
    "    IN: month = start month (September: 9), current_url = first url to search from, stop_month = end month (May: 5)\n",
    "    OUT: dictionary of relevant articles and their attributes\n",
    "    '''\n",
    "    \n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(current_url)\n",
    "    \n",
    "    while month != stop_month:\n",
    "        \n",
    "        try:\n",
    "            first_link_date = get_links(current_url)\n",
    "            date = get_date(first_link_date) + timedelta(days=1)\n",
    "            search_date = date.strftime('%m/%d/%Y')\n",
    "            month = date.month\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                driver.find_element_by_xpath('//a[text()=\"Next\"]').click()\n",
    "                current_url = driver.current_url\n",
    "                \n",
    "            except:\n",
    "            \n",
    "                date = driver.find_element_by_id('bdv')\n",
    "                date.clear()\n",
    "                date.send_keys(search_date)\n",
    "                driver.find_element_by_xpath('//input[@value=\"Search\"]').click()\n",
    "                current_url = driver.current_url\n",
    "        \n",
    "        except:   \n",
    "            date = driver.find_element_by_id('bdv')\n",
    "            date.clear()\n",
    "            date.send_keys(search_date)\n",
    "            driver.find_element_by_xpath('//input[@value=\"Search\"]').click()\n",
    "            current_url = driver.current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call function to get all/relevant links for 4 months\n",
    "## (Verify dictionary in get_links function is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the links dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continuous_search(9, current_url, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('links.pkl', 'wb') as f:\n",
    "    pickle.dump(links, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize fed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fed_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function for Fed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_search(9, 'http://www.marketwatch.com/search?q=&m=Section&rpp=100&mp=Economy+%26+Politics%7CFederal+Reserve&bd=true&bd=false&bdv=09%2F30%2F2017&rs=true', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('fed.pkl', 'wb') as f:\n",
    "    pickle.dump(fed_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize economics dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "econ = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function for Econ articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_search(9, 'http://www.marketwatch.com/search?q=&m=Subject&rpp=100&mp=ECAT&bd=true&bd=false&bdv=09%2F30%2F2017&rs=true', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('econ.pkl', 'wb') as f:\n",
    "    pickle.dump(econ, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Determine which econ articles are related to the Fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "econlist = [x for x in econ.items()]\n",
    "\n",
    "def search_links(econlist=econlist):\n",
    "    '''\n",
    "    This function searches every economics article and determines if it is related to the Federal Resereve\n",
    "    \"Strong\" cases are when the words 'yellen', 'fed', 'federal', 'reserve', or 'fomc' appear in the title\n",
    "    \"Soft\" or weak cases are when the above words (sans 'federal') appear anywhere in the article\n",
    "    ---\n",
    "    IN: list of economic articles/links (econlist)\n",
    "    OUT: strong = list of article info for strong cases, soft = list of article infor weak cases\n",
    "    '''\n",
    "    strong = []\n",
    "    soft = []\n",
    "    for link in econlist:\n",
    "        \n",
    "        title_count = 0\n",
    "        article_count = 0\n",
    "        \n",
    "        title = link[0]\n",
    "        \n",
    "        for word in title.lower().split():\n",
    "            word = re.sub(r'[%s]' % re.escape(string.punctuation), '', word)\n",
    "            word = re.search(r'(\\w*)(\\'s)?', word)[1]\n",
    "            \n",
    "            if word in ['yellen', 'fed', 'federal', 'reserve', 'fomc']:\n",
    "                title_count += 1\n",
    "                \n",
    "        if title_count > 0:\n",
    "            strong.append(link)\n",
    "        \n",
    "        else:\n",
    "            url = link[1]['url']\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "            paragraphs = soup.find(id='article-body').find_all('p')\n",
    "            for para in [t.lower().split() for t in [''.join(p.find_all(text=True)) for p in paragraphs if bool(p.findChildren('strong')) == False]]:\n",
    "                for word in para:\n",
    "                    if word in ['yellen', 'fed', 'reserve', 'fomc']:\n",
    "                        article_count += 1\n",
    "            if article_count > 0:\n",
    "                soft.append(link)\n",
    "                \n",
    "    return strong, soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strong, soft = search_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('strong.pkl', 'wb') as f:\n",
    "    pickle.dump(strong, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('soft.pkl', 'wb') as f:\n",
    "    pickle.dump(soft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
